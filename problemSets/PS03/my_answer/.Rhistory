comp_tok <- tokens_compound(lemma_toks, keep_coll_list)
dfm_dprk <- dfm(comp_tok)
saveRDS(dfm_dprk, "C:/Users/tangw/Desktop/QTA final project/dprk_data")
saveRDS(dfm_dprk, "C:/Users/tangw/Desktop/QTA final project/dprk_data")
topfeatures(dfm_dprk)
dfm_dprk %>%
dfm_trim(min_termfreq = 3) %>%
textplot_wordcloud(min_size = 1, max_size = 5, max_words = 150)
#Pre-processing
# tokenize the text
toks <- quanteda::tokens(corpus_dprk,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE)
# lowercase the text
toks <- tokens_tolower(toks)
# remove stop words
stop_list <- stopwords("english")
toks <- tokens_remove(toks, stop_list)
# lemmatize
stem_toks <- tokens_wordstem(toks)
toks_list <- as.list(toks)
lemma_toks <- lapply(toks_list, lemmatize_words)
# Detect collocations
collocations <- textstat_collocations(lemma_toks, size = 2)
comp_tok <- tokens_compound(lemma_toks, collocations)
# lemmatize
stem_toks <- tokens_wordstem(toks)
toks_list <- as.list(toks)
lemma_toks <- lapply(toks_list, lemmatize_words)
lemma_toks <- as.tokens(lemma_toks)
# Detect collocations
collocations <- textstat_collocations(lemma_toks, size = 2)
# Detect collocations
collocations <- textstat_collocations(lemma_toks, size = 2)
comp_tok <- tokens_compound(lemma_toks, collocations)
# create the document-feature matrix (dfm)
dfm_dprk <- dfm(comp_tok)
# save
saveRDS(dfm_dprk, "C:/Users/tangw/Desktop/QTA final project/dprk_data")
# look through the top features
topfeatures(dfm_dprk)
# visualise the dfm using the textplot
dfm_dprk %>%
dfm_trim(min_termfreq = 5) %>%
textplot_wordcloud(min_size = 1, max_size = 5, max_words = 150)
# visualise the dfm using the textplot
dfm_dprk %>%
dfm_trim(min_termfreq = 5) %>%
textplot_wordcloud(min_size = 1, max_size = 6, max_words = 100)
# visualise the dfm using the textplot
dfm_dprk %>%
dfm_trim(min_termfreq = 5) %>%
textplot_wordcloud(min_size = 1, max_size = 3, max_words = 100)
# visualise the dfm using the textplot
dfm_dprk %>%
dfm_trim(min_termfreq = 3) %>%
textplot_wordcloud(min_size = 1, max_size = 3, max_words = 100)
#Pre-processing
# tokenize the text
toks <- quanteda::tokens(corpus_dprk,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE)
# lowercase the text
toks <- tokens_tolower(toks)
# remove stop words
stop_list <- stopwords("english")
toks <- tokens_remove(toks, stop_list)
# lemmatize
stem_toks <- tokens_wordstem(toks)
toks_list <- as.list(toks)
lemma_toks <- lapply(toks_list, lemmatize_words)
# Detect collocations
collocations <- textstat_collocations(lemma_toks, size = 2)
# lemmatize
stem_toks <- tokens_wordstem(toks)
toks_list <- as.list(toks)
lemma_toks <- lapply(toks_list, lemmatize_words)
lemma_toks <- lapply(toks_list, lemmatize_words)
lemma_toks <- as.tokens(lemma_toks)
# Detect collocations
collocations <- textstat_collocations(lemma_toks, size = 2)
keep_coll_list <- collocations$collocation[1:20]
comp_tok <- tokens_compound(lemma_toks, keep_coll_list)
# create the document-feature matrix (dfm)
dfm_dprk <- dfm(comp_tok)
# save
saveRDS(dfm_dprk, "C:/Users/tangw/Desktop/QTA final project/dprk_data")
# look through the top features
topfeatures(dfm_dprk)
# visualise the dfm using the textplot
dfm_dprk %>%
dfm_trim(min_termfreq = 3) %>%
textplot_wordcloud(min_size = 1, max_size = 5, max_words = 150)
#Pre-processing
# tokenize the text
toks <- quanteda::tokens(corpus_dprk,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_hyphens = TRUE,
remove_separators = TRUE,
remove_url = TRUE,
include_docvars = TRUE)
# lowercase the text
toks <- tokens_tolower(toks)
# remove stop words
stop_list <- stopwords("english")
toks <- tokens_remove(toks, stop_list)
# lemmatize
stem_toks <- tokens_wordstem(toks)
toks_list <- as.list(toks)
lemma_toks <- lapply(toks_list, lemmatize_words)
lemma_toks <- as.tokens(lemma_toks)
# Detect collocations
collocations <- textstat_collocations(lemma_toks, size = 2)
keep_coll_list <- collocations$collocation[1:20]
comp_tok <- tokens_compound(lemma_toks, keep_coll_list)
# create the document-feature matrix (dfm)
dfm_dprk <- dfm(comp_tok)
# save
saveRDS(dfm_dprk, "C:/Users/tangw/Desktop/QTA final project/dprk_data")
# look through the top features
topfeatures(dfm_dprk)
# visualise the dfm using the textplot
dfm_dprk %>%
dfm_trim(min_termfreq = 3) %>%
textplot_wordcloud(min_size = 1, max_size = 5, max_words = 150)
dprk_dfm <- corpus_dprk %>%
quanteda::tokens(remove_numbers = TRUE,
remove_punct = TRUE) %>%
dfm()
sample(data_dictionary_LSD2015$negative, 10)
sample(data_dictionary_LSD2015$positive, 10)
# create sensity dfm
dprk_sent_dfm <- dfm_lookup(dprk_dfm,
dictionary = data_dictionary_LSD2015[1:2])
head(dprk_sent_dfm, 5)
#View(corpus_dprk)
docvars(dprk_dfm, "prop_negative") <- as.numeric(dprk_sent_dfm[,1] / ntoken(dprk_dfm)) # add proportion of negative words to original dfm
docvars(dprk_dfm, "prop_positive") <- as.numeric(dprk_sent_dfm[,2] / ntoken(dprk_dfm))
docvars(dprk_dfm, "net_sentiment") <- docvars(dprk_dfm, "prop_positive") - docvars(dprk_dfm, "prop_negative") # calculate net sentiment
library(ggplot2)
# create diplomacy points
diplomacy_df <- data.frame(
# dates when the activity occur
date = as.Date(c("2018-01-01", "2018-01-03", "2018-01-09", "2018-01-20",
"2018-02-08", "2018-02-09", "2018-03-05", "2018-03-28",
"2018-04-27", "2018-05-02", "2018-05-12", "2018-05-15",
"2018-05-24", "2018-06-12", "2018-09-09", "2018-09-18",
"2018-11-04", "2018-12-03")),
# positive or negative activity to diplomatic relationship
diplomacy = c(1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0)*0.01-0.005,
# month of the date
month = format(as.Date(c("2018-01-01", "2018-01-03", "2018-01-09", "2018-01-20",
"2018-02-08", "2018-02-09", "2018-03-05", "2018-03-28",
"2018-04-27", "2018-05-02", "2018-05-12", "2018-05-15",
"2018-05-24", "2018-06-12", "2018-09-09", "2018-09-18",
"2018-11-04", "2018-12-03")), "%Y-%m")
)
dprk_dfm_df <- convert(dprk_dfm, to = "data.frame")
# transform the date format
dprk_dfm$date <- as.Date(dprk_dfm$date, format = "%Y-%m-%d")
# create the plot
combined_plot <- ggplot() +
# add smooth curves
geom_smooth(data = docvars(dprk_dfm), aes(x = date, y = net_sentiment), color = "blue") +
# add scatters
geom_smooth(data = diplomacy_df, aes(x = date, y = diplomacy), color = "red") +
geom_point(data = diplomacy_df, aes(x = date, y = diplomacy), color = "red") +
labs(title = "Comparison of Net Sentiment and Diplomacy",
x = "Date",
y = "Value") +
theme_minimal()
# print the plot
print(combined_plot)
# create toks
toks <- quanteda::tokens(corpus_dprk,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE)
# create tok keys
toks_key <- tokens_keep(toks,
pattern = 'korea',
valuetype = "glob",
window = 15) # get window of words around keyword
# create dfm of tokens object
toks_key <- dfm(toks_key)
toks_key_lsd <- toks_key %>%
dfm_lookup(dictionary = data_dictionary_LSD2015[1:2]) %>%
dfm_group(group = week, fill = TRUE)
# plot net sentiment
n_toks <- ntoken(dfm_group(toks_key,
group = docvars(toks_key, 'week'))) # get number of weekly tokens
plot((toks_key_lsd[,2] - toks_key_lsd[,1]) / n_toks,
type = 'l', ylab = "Net Sentiment", xlab = '', main = 'Net sentiment around "korea"', xaxt = "n") # plot difference in share of positive and negative terms
axis(1, seq_len(ndoc(toks_key_lsd)), ymd("2018-01-01") + weeks(seq_len(ndoc(toks_key_lsd)) - 1))
grid()
abline(h = 0, lty = 2)
# create tok keys
toks_key <- tokens_keep(toks,
pattern = 'nuclear',
valuetype = "glob",
window = 15) # get window of words around keyword
# create dfm of tokens object
toks_key <- dfm(toks_key)
toks_key_lsd <- toks_key %>%
dfm_lookup(dictionary = data_dictionary_LSD2015[1:2]) %>%
dfm_group(group = week, fill = TRUE)
# plot net sentiment
n_toks <- ntoken(dfm_group(toks_key,
group = docvars(toks_key, 'week'))) # get number of weekly tokens
plot((toks_key_lsd[,2] - toks_key_lsd[,1]) / n_toks,
type = 'l', ylab = "Net Sentiment", xlab = '', main = 'Net sentiment around "nuclear"', xaxt = "n") # plot difference in share of positive and negative terms
axis(1, seq_len(ndoc(toks_key_lsd)), ymd("2018-01-01") + weeks(seq_len(ndoc(toks_key_lsd)) - 1))
grid()
abline(h = 0, lty = 2)
# create tok keys
toks_key <- tokens_keep(toks,
pattern = 'security',
valuetype = "glob",
window = 15) # get window of words around keyword
# create dfm of tokens object
toks_key <- dfm(toks_key)
toks_key_lsd <- toks_key %>%
dfm_lookup(dictionary = data_dictionary_LSD2015[1:2]) %>%
dfm_group(group = week, fill = TRUE)
# plot net sentiment
n_toks <- ntoken(dfm_group(toks_key,
group = docvars(toks_key, 'week'))) # get number of weekly tokens
plot((toks_key_lsd[,2] - toks_key_lsd[,1]) / n_toks,
type = 'l', ylab = "Net Sentiment", xlab = '', main = 'Net sentiment around "security"', xaxt = "n") # plot difference in share of positive and negative terms
axis(1, seq_len(ndoc(toks_key_lsd)), ymd("2018-01-01") + weeks(seq_len(ndoc(toks_key_lsd)) - 1))
grid()
abline(h = 0, lty = 2)
knitr::opts_chunk$set(echo = TRUE)
## Load packages
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("tidyverse",
"guardianapi", # use the Guardian's API
"quanteda",
"quanteda.textstats",
"quanteda.textplots",
"readtext",
"stringi",
"textstem",
"dplyr",
"lubridate",
"readtext"
), pkgTest)
# use the Guardian API
gu_api_key()
# get the dataset of articles about North Korea during 2018
dprk_data <- gu_content(query = "North Korea", from_date = "2018-01-01", to_date = "2019-01-01")
# save the dataset
saveRDS(dprk_data, "C:/Users/tangw/Desktop/QTA final project/dprk_data")
# make a copy of the dataset
dprk_data_copy <- dprk_data
View(dprk_data)
# copy the first publication date as a date column
dprk_data$date <- as.Date(dprk_data$first_publication_date, format = "%Y-%m-%d")
# detect invalid dates
invalid_dates <- dprk_data[is.na(as.Date(dprk_data$first_publication_date, format = "%Y-%m-%d")), ]
# show how many invalid dates
print(nrow(invalid_dates))
# add a column as week based on the date
dprk_data$week <- week(dprk_data$first_publication_date)
# only get the article in 2018
dprk_data <- dprk_data[dprk_data$date >= as.Date("2018-01-01"), ]
dprk_data <- dprk_data[dprk_data$date <= as.Date("2019-01-01"), ]
# make copy
df<-dprk_data
# filter the article type and world section
dprk_data <- dprk_data[dprk_data$type == "article" & df$section_id == "world",]
# move duplicates
dprk_data <- dprk_data[!duplicated(dprk_data$web_title),]
View(dprk_data)
# build the corpus
corpus_dprk <- corpus(dprk_data,
docid_field = "web_title",
text_field = "body_text")
View(corpus_dprk)
#Pre-processing
# tokenize the text
toks <- quanteda::tokens(corpus_dprk,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_hyphens = TRUE,
remove_separators = TRUE,
remove_url = TRUE,
include_docvars = TRUE)
# lowercase the text
toks <- tokens_tolower(toks)
# remove stop words
stop_list <- stopwords("english")
toks <- tokens_remove(toks, stop_list)
# lemmatize
stem_toks <- tokens_wordstem(toks)
toks_list <- as.list(toks)
lemma_toks <- lapply(toks_list, lemmatize_words)
lemma_toks <- as.tokens(lemma_toks)
# Detect collocations
collocations <- textstat_collocations(lemma_toks, size = 2)
keep_coll_list <- collocations$collocation[1:20]
comp_tok <- tokens_compound(lemma_toks, keep_coll_list)
# create the document-feature matrix (dfm)
dfm_dprk <- dfm(comp_tok)
# save
saveRDS(dfm_dprk, "C:/Users/tangw/Desktop/QTA final project/dprk_data")
# look through the top features
topfeatures(dfm_dprk)
# visualise the dfm using the textplot
dfm_dprk %>%
dfm_trim(min_termfreq = 3) %>%
textplot_wordcloud(min_size = 1, max_size = 5, max_words = 150)
dprk_dfm <- corpus_dprk %>%
quanteda::tokens(remove_numbers = TRUE,
remove_punct = TRUE) %>%
dfm()
sample(data_dictionary_LSD2015$negative, 10)
sample(data_dictionary_LSD2015$positive, 10)
# create sensity dfm
dprk_sent_dfm <- dfm_lookup(dprk_dfm,
dictionary = data_dictionary_LSD2015[1:2])
head(dprk_sent_dfm, 5)
#View(corpus_dprk)
docvars(dprk_dfm, "prop_negative") <- as.numeric(dprk_sent_dfm[,1] / ntoken(dprk_dfm)) # add proportion of negative words to original dfm
docvars(dprk_dfm, "prop_positive") <- as.numeric(dprk_sent_dfm[,2] / ntoken(dprk_dfm))
docvars(dprk_dfm, "net_sentiment") <- docvars(dprk_dfm, "prop_positive") - docvars(dprk_dfm, "prop_negative") # calculate net sentiment
library(ggplot2)
# create diplomacy points
diplomacy_df <- data.frame(
# dates when the activity occur
date = as.Date(c("2018-01-01", "2018-01-03", "2018-01-09", "2018-01-20",
"2018-02-08", "2018-02-09", "2018-03-05", "2018-03-28",
"2018-04-27", "2018-05-02", "2018-05-12", "2018-05-15",
"2018-05-24", "2018-06-12", "2018-09-09", "2018-09-18",
"2018-11-04", "2018-12-03")),
# positive or negative activity to diplomatic relationship
diplomacy = c(1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0)*0.01-0.005,
# month of the date
month = format(as.Date(c("2018-01-01", "2018-01-03", "2018-01-09", "2018-01-20",
"2018-02-08", "2018-02-09", "2018-03-05", "2018-03-28",
"2018-04-27", "2018-05-02", "2018-05-12", "2018-05-15",
"2018-05-24", "2018-06-12", "2018-09-09", "2018-09-18",
"2018-11-04", "2018-12-03")), "%Y-%m")
)
dprk_dfm_df <- convert(dprk_dfm, to = "data.frame")
# transform the date format
dprk_dfm$date <- as.Date(dprk_dfm$date, format = "%Y-%m-%d")
# create the plot
combined_plot <- ggplot() +
# add smooth curves
geom_smooth(data = docvars(dprk_dfm), aes(x = date, y = net_sentiment), color = "blue") +
# add scatters
geom_smooth(data = diplomacy_df, aes(x = date, y = diplomacy), color = "red") +
geom_point(data = diplomacy_df, aes(x = date, y = diplomacy), color = "red") +
labs(title = "Comparison of Net Sentiment and Diplomacy",
x = "Date",
y = "Value") +
theme_minimal()
# print the plot
print(combined_plot)
# create toks
toks <- quanteda::tokens(corpus_dprk,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE)
# create tok keys
toks_key <- tokens_keep(toks,
pattern = 'korea',
valuetype = "glob",
window = 15) # get window of words around keyword
# create dfm of tokens object
toks_key <- dfm(toks_key)
toks_key_lsd <- toks_key %>%
dfm_lookup(dictionary = data_dictionary_LSD2015[1:2]) %>%
dfm_group(group = week, fill = TRUE)
# plot net sentiment
n_toks <- ntoken(dfm_group(toks_key,
group = docvars(toks_key, 'week'))) # get number of weekly tokens
plot((toks_key_lsd[,2] - toks_key_lsd[,1]) / n_toks,
type = 'l', ylab = "Net Sentiment", xlab = '', main = 'Net sentiment around "korea"', xaxt = "n") # plot difference in share of positive and negative terms
axis(1, seq_len(ndoc(toks_key_lsd)), ymd("2018-01-01") + weeks(seq_len(ndoc(toks_key_lsd)) - 1))
grid()
abline(h = 0, lty = 2)
# create tok keys
toks_key <- tokens_keep(toks,
pattern = 'nuclear',
valuetype = "glob",
window = 15) # get window of words around keyword
# create dfm of tokens object
toks_key <- dfm(toks_key)
toks_key_lsd <- toks_key %>%
dfm_lookup(dictionary = data_dictionary_LSD2015[1:2]) %>%
dfm_group(group = week, fill = TRUE)
# plot net sentiment
n_toks <- ntoken(dfm_group(toks_key,
group = docvars(toks_key, 'week'))) # get number of weekly tokens
plot((toks_key_lsd[,2] - toks_key_lsd[,1]) / n_toks,
type = 'l', ylab = "Net Sentiment", xlab = '', main = 'Net sentiment around "nuclear"', xaxt = "n") # plot difference in share of positive and negative terms
axis(1, seq_len(ndoc(toks_key_lsd)), ymd("2018-01-01") + weeks(seq_len(ndoc(toks_key_lsd)) - 1))
grid()
abline(h = 0, lty = 2)
# create tok keys
toks_key <- tokens_keep(toks,
pattern = 'security',
valuetype = "glob",
window = 15) # get window of words around keyword
# create dfm of tokens object
toks_key <- dfm(toks_key)
toks_key_lsd <- toks_key %>%
dfm_lookup(dictionary = data_dictionary_LSD2015[1:2]) %>%
dfm_group(group = week, fill = TRUE)
# plot net sentiment
n_toks <- ntoken(dfm_group(toks_key,
group = docvars(toks_key, 'week'))) # get number of weekly tokens
plot((toks_key_lsd[,2] - toks_key_lsd[,1]) / n_toks,
type = 'l', ylab = "Net Sentiment", xlab = '', main = 'Net sentiment around "security"', xaxt = "n") # plot difference in share of positive and negative terms
axis(1, seq_len(ndoc(toks_key_lsd)), ymd("2018-01-01") + weeks(seq_len(ndoc(toks_key_lsd)) - 1))
grid()
abline(h = 0, lty = 2)
# check the summary of the model
summary(mod.ps)
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics","package:nnet", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base","package:AER","package:pscl")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("nnet", "MASS","pscl","AER"),  pkgTest)
# set wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# load and read data
gdp_data <- read.csv("https://raw.githubusercontent.com/ASDS-TCD/StatsII_Spring2024/main/datasets/gdpChange.csv", stringsAsFactors = F)
# create a new column called GDP_change, including categories:"increase","decrease","no_change".
# GDP_change is depending on GDPWdiff.
for (i in 1:length(gdp_data$GDPWdiff)) {
if (gdp_data$GDPWdiff[i] > 0) {
gdp_data$GDP_change[i] <- "increase"
} else if (gdp_data$GDPWdiff[i] < 0) {
gdp_data$GDP_change[i] <- "decrease"
} else {
gdp_data$GDP_change[i] <- "no_change"
}
}
# factorize the response variables and explanatory variables, put their levels and labels in the same order.
gdp_data$GDP_change <- factor(gdp_data$GDP_change, levels = c("decrease","no_change","increase"),labels = c("decrease","no_change","increase"))
gdp_data$OIL <- factor(gdp_data$OIL, levels = c(0,1),labels = c("otherwise","Exceed 50%"))
gdp_data$REG <- factor(gdp_data$REG, levels = c(0,1),labels = c("Non-Democracy","Democracy"))
# unordered model
# relevel GDP_change and set "no_change" as the reference category.
gdp_data$GDP_change <- relevel(gdp_data$GDP_change,ref = "no_change")
# run a unordered model
unordered_mod <- multinom(GDP_change ~ REG + OIL, data = gdp_data)
# get the summary of the model to get coefficients
summary(unordered_mod)
# exponent e by coefficients
exp(coef(unordered_mod))
# ordered model
# run an ordered model
ordered_mod <- polr(GDP_change ~ REG + OIL, data = gdp_data)
# get the summary of the model to get coefficients
summary(ordered_mod)
# exponent e by coefficients
exp(coef(ordered_mod))
# get the confidence interval
exp(cbind(OR=coef(ordered_mod),confint(ordered_mod)))
# load and read data
mexico_elections <- read.csv("https://raw.githubusercontent.com/ASDS-TCD/StatsII_Spring2024/main/datasets/MexicoMuniData.csv")
# run Poisson regression and build model
mod.ps <- glm(PAN.visits.06 ~ competitive.district + marginality.06 + PAN.governor.06, data = mexico_elections, family = poisson)
# check the summary of the model
summary(mod.ps)
# do dispersion test
dispersiontest(mod.ps)
# build a zero-inflation model
mod.zip <- zeroinfl(PAN.visits.06 ~ competitive.district + marginality.06 + PAN.governor.06, data = mexico_elections, dist = "poisson")
# check the summary of the model
summary(mod.zip)
# withdraw coefficients
cfs <- coef(mod.zip)
# predict the d mean number of visits
pre_data <- data.frame(competitive.district = 1, marginality.06 = 0, PAN.governor.06 = 1)
# get the estimate from original Poisson regression model
exp(predict(mod.ps, newdata = pre_data))
# get the estimate from zero-inflation model
exp(predict(mod.zip, newdata = pre_data))
# check the summary of the model
summary(mod.zip)
